{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e996881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Cityscapes\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "image_size = (64, 128)\n",
    "root_dir = \"datasets/cityscapes\"\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize(\n",
    "        image_size,\n",
    "        interpolation=transforms.InterpolationMode.NEAREST\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "full_train_dataset = Cityscapes(\n",
    "    root=root_dir,\n",
    "    split='train',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    transform=input_transform,\n",
    "    target_transform=target_transform,\n",
    ")\n",
    "\n",
    "full_val_dataset = Cityscapes(\n",
    "    root=root_dir,\n",
    "    split='val',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    transform=input_transform,\n",
    "    target_transform=target_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b4514c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Cityscapes.CityscapesWrapper import CityscapesWrapper\n",
    "\n",
    "train_samples = 800\n",
    "val_samples = 200\n",
    "\n",
    "train_subset = CityscapesWrapper(\n",
    "    Subset(\n",
    "        full_train_dataset,\n",
    "        range(train_samples)\n",
    "    ),\n",
    "    target_transform=target_transform\n",
    ")\n",
    "val_subset = CityscapesWrapper(\n",
    "    Subset(\n",
    "        full_val_dataset,\n",
    "        range(val_samples)\n",
    "    ),\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec35f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Cityscapes.UNetTorchCityscapes import UNet\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"mps\"\n",
    "model = UNet(\n",
    "    in_channels=3,\n",
    "    out_channels=34\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050cda06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [01:03<00:00,  3.17it/s]\n",
      "Validation: 100%|██████████| 50/50 [00:13<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0081 | Val Loss: 1.1947\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [01:02<00:00,  3.20it/s]\n",
      "Validation: 100%|██████████| 50/50 [00:13<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1861 | Val Loss: 1.0645\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [01:02<00:00,  3.19it/s]\n",
      "Validation: 100%|██████████| 50/50 [00:13<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0937 | Val Loss: 1.0635\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [01:02<00:00,  3.20it/s]\n",
      "Validation: 100%|██████████| 50/50 [00:13<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0178 | Val Loss: 0.9393\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [01:02<00:00,  3.19it/s]\n",
      "Validation: 100%|██████████| 50/50 [00:13<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9518 | Val Loss: 0.9309\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 86/200 [00:26<00:35,  3.18it/s]"
     ]
    }
   ],
   "source": [
    "from utils.Cityscapes.CityscapesTrainingUtils import train_one_epoch_cityscapes, validate_cityscapes\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_one_epoch_cityscapes(model, train_loader, optimizer, criterion)\n",
    "    val_loss = validate_cityscapes(model, val_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Val Loss', marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb08231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_image, sample_target = next(iter(val_loader))\n",
    "    sample_image = sample_image.to(device)\n",
    "    sample_target = sample_target.to(device)\n",
    "\n",
    "    output = model(sample_image)\n",
    "    pred_mask = torch.argmax(output, dim=1)\n",
    "\n",
    "img = sample_image[0].cpu().permute(1, 2, 0).numpy()\n",
    "gt_mask = sample_target[0].cpu().squeeze().numpy()\n",
    "pred_mask = pred_mask[0].cpu().numpy()\n",
    "\n",
    "def apply_colormap(mask):\n",
    "    colormap = np.array([\n",
    "        [0, 0, 0],         # class 0: black\n",
    "        [0, 255, 0],       # class 1: green\n",
    "        [0, 0, 255],       # class 2: blue\n",
    "        [255, 0, 0],       # class 3: red\n",
    "        [255, 255, 0],     # etc.\n",
    "    ])\n",
    "    mask_rgb = colormap[mask % len(colormap)]\n",
    "    return mask_rgb.astype(np.uint8)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].imshow(img)\n",
    "axs[0].set_title(\"Input Image\")\n",
    "axs[1].imshow(apply_colormap(gt_mask))\n",
    "axs[1].set_title(\"Ground Truth Mask\")\n",
    "axs[2].imshow(apply_colormap(pred_mask))\n",
    "axs[2].set_title(\"Predicted Mask\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81971faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./weights/cityscapes_unet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66088cc",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 34\n",
    "\n",
    "model1 = UNet(in_channels=3, out_channels=num_classes).to(device)\n",
    "model2 = UNet(in_channels=3, out_channels=num_classes).to(device)\n",
    "model3 = UNet(in_channels=3, out_channels=num_classes).to(device)\n",
    "\n",
    "ensemble_models = [model1, model2, model3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d2f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=1e-3)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
    "optimizer3 = torch.optim.Adam(model3.parameters(), lr=1e-3)\n",
    "\n",
    "ensemble_optimizers = [optimizer1, optimizer2, optimizer3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch_ensemble(models, optimizers, loader, criterion):\n",
    "    for model in models:\n",
    "        model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, targets in tqdm(loader, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device).long().squeeze(1)\n",
    "\n",
    "        batch_loss = 0\n",
    "\n",
    "        for model, optimizer in zip(models, optimizers):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        total_loss += batch_loss / len(models)  # Average loss over ensemble\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate_ensemble(models, loader, criterion):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device).long().squeeze(1)\n",
    "\n",
    "            # Get ensemble predictions (softmax average)\n",
    "            outputs = [torch.softmax(model(images), dim=1) for model in models]\n",
    "            avg_output = torch.stack(outputs).mean(dim=0)\n",
    "\n",
    "            loss = criterion(avg_output, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_one_epoch_ensemble(ensemble_models, ensemble_optimizers, train_loader, criterion)\n",
    "    val_loss = validate_ensemble(ensemble_models, val_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(ensemble_models):\n",
    "    torch.save(model.state_dict(), f\"./weights/cityscapes_ensemble_unet_{i+1}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
