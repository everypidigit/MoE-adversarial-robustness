{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef7516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Cityscapes\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "image_size = (64, 128)\n",
    "root_dir = \"datasets/cityscapes\"\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize(\n",
    "        image_size,\n",
    "        interpolation=transforms.InterpolationMode.NEAREST\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "full_train_dataset = Cityscapes(\n",
    "    root=root_dir,\n",
    "    split='train',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    transform=input_transform,\n",
    "    target_transform=target_transform,\n",
    ")\n",
    "\n",
    "full_val_dataset = Cityscapes(\n",
    "    root=root_dir,\n",
    "    split='val',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    transform=input_transform,\n",
    "    target_transform=target_transform,\n",
    ")\n",
    "\n",
    "from utils.Cityscapes.CityscapesWrapper import CityscapesWrapper\n",
    "\n",
    "train_samples = 800\n",
    "val_samples = 200\n",
    "\n",
    "train_subset = CityscapesWrapper(\n",
    "    Subset(\n",
    "        full_train_dataset,\n",
    "        range(train_samples)\n",
    "    ),\n",
    "    target_transform=target_transform\n",
    ")\n",
    "val_subset = CityscapesWrapper(\n",
    "    Subset(\n",
    "        full_val_dataset,\n",
    "        range(val_samples)\n",
    "    ),\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8534b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchattacks\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils.Cityscapes.UNetTorchCityscapes import UNet\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "single_unet = UNet(in_channels=3, out_channels=34)\n",
    "single_unet.load_state_dict(torch.load(\"./weights/cityscapes_unet.pth\"))\n",
    "single_unet.to(device)\n",
    "single_unet.eval()\n",
    "\n",
    "ensemble_models = []\n",
    "for i in range(3):\n",
    "    ensemble_model = UNet(in_channels=3, out_channels=34)\n",
    "    ensemble_model.load_state_dict(torch.load(f\"./weights/cityscapes_ensemble_unet_{i+1}.pth\"))\n",
    "    ensemble_model.to(device)\n",
    "    ensemble_model.eval()\n",
    "    ensemble_models.append(ensemble_model)\n",
    "\n",
    "def apply_colormap(mask):\n",
    "    colormap = np.array([\n",
    "        [0, 0, 0],         # class 0: black\n",
    "        [0, 255, 0],       # class 1: green\n",
    "        [0, 0, 255],       # class 2: blue\n",
    "        [255, 0, 0],       # class 3: red\n",
    "        [255, 255, 0],     # etc.\n",
    "    ])\n",
    "    mask_rgb = colormap[mask % len(colormap)]\n",
    "    return mask_rgb.astype(np.uint8)\n",
    "\n",
    "sample_image, sample_target = next(iter(val_loader))\n",
    "sample_image = sample_image.to(device)\n",
    "sample_target = sample_target.to(device).long().squeeze(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    single_unet_output = single_unet(sample_image)\n",
    "    single_unet_pred = torch.argmax(single_unet_output, dim=1)\n",
    "\n",
    "ensemble_preds = [torch.softmax(m(sample_image), dim=1) for m in ensemble_models]\n",
    "ensemble_avg_pred = torch.stack(ensemble_preds).mean(dim=0)\n",
    "ensemble_pred = torch.argmax(ensemble_avg_pred, dim=1)\n",
    "\n",
    "fgsm = torchattacks.FGSM(single_unet, eps=0.02)\n",
    "pgd = torchattacks.PGD(single_unet, eps=0.02, alpha=0.01, steps=40)\n",
    "\n",
    "\n",
    "sample_fgsm = fgsm(sample_image, sample_target)\n",
    "sample_pgd = pgd(sample_image, sample_target)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    fgsm_single_output = single_unet(sample_fgsm)\n",
    "    fgsm_single_pred = torch.argmax(fgsm_single_output, dim=1)\n",
    "\n",
    "    pgd_single_output = single_unet(sample_pgd)\n",
    "    pgd_single_pred = torch.argmax(pgd_single_output, dim=1)\n",
    "\n",
    "\n",
    "fgsm_ensemble_preds = [torch.softmax(m(sample_fgsm), dim=1) for m in ensemble_models]\n",
    "fgsm_ensemble_avg_pred = torch.stack(fgsm_ensemble_preds).mean(dim=0)\n",
    "fgsm_ensemble_pred = torch.argmax(fgsm_ensemble_avg_pred, dim=1)\n",
    "\n",
    "pgd_ensemble_preds = [torch.softmax(m(sample_pgd), dim=1) for m in ensemble_models]\n",
    "pgd_ensemble_avg_pred = torch.stack(pgd_ensemble_preds).mean(dim=0)\n",
    "pgd_ensemble_pred = torch.argmax(pgd_ensemble_avg_pred, dim=1)\n",
    "\n",
    "img = sample_image[0].cpu().permute(1, 2, 0).numpy()\n",
    "gt_mask = sample_target[0].cpu().squeeze().numpy()\n",
    "single_unet_mask = single_unet_pred[0].cpu().numpy()\n",
    "ensemble_mask = ensemble_pred[0].cpu().numpy()\n",
    "fgsm_single_mask = fgsm_single_pred[0].cpu().numpy()\n",
    "pgd_single_mask = pgd_single_pred[0].cpu().numpy()\n",
    "fgsm_ensemble_mask = fgsm_ensemble_pred[0].cpu().numpy()\n",
    "pgd_ensemble_mask = pgd_ensemble_pred[0].cpu().numpy()\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(18, 9))\n",
    "\n",
    "# Original Image\n",
    "axs[0, 0].imshow(img)\n",
    "axs[0, 0].set_title(\"Input Image\")\n",
    "axs[0, 0].axis('off')\n",
    "\n",
    "# Ground Truth Mask\n",
    "axs[0, 1].imshow(apply_colormap(gt_mask))\n",
    "axs[0, 1].set_title(\"Ground Truth Mask\")\n",
    "axs[0, 1].axis('off')\n",
    "\n",
    "# Mask with Single UNet\n",
    "axs[0, 2].imshow(apply_colormap(single_unet_mask))\n",
    "axs[0, 2].set_title(\"Single UNet Prediction\")\n",
    "axs[0, 2].axis('off')\n",
    "\n",
    "# Mask with Ensemble UNet\n",
    "axs[0, 3].imshow(apply_colormap(ensemble_mask))\n",
    "axs[0, 3].set_title(\"Ensemble UNet Prediction\")\n",
    "axs[0, 3].axis('off')\n",
    "\n",
    "# Mask with FGSM Attack and Single UNet\n",
    "axs[1, 0].imshow(apply_colormap(fgsm_single_mask))\n",
    "axs[1, 0].set_title(\"FGSM Attack (Single UNet)\")\n",
    "axs[1, 0].axis('off')\n",
    "\n",
    "# Mask with PGD Attack and Single UNet\n",
    "axs[1, 1].imshow(apply_colormap(pgd_single_mask))\n",
    "axs[1, 1].set_title(\"PGD Attack (Single UNet)\")\n",
    "axs[1, 1].axis('off')\n",
    "\n",
    "# Mask with FGSM Attack and Ensemble UNet\n",
    "axs[1, 2].imshow(apply_colormap(fgsm_ensemble_mask))\n",
    "axs[1, 2].set_title(\"FGSM Attack (Ensemble UNet)\")\n",
    "axs[1, 2].axis('off')\n",
    "\n",
    "# Mask with PGD Attack and Ensemble UNet\n",
    "axs[1, 3].imshow(apply_colormap(pgd_ensemble_mask))\n",
    "axs[1, 3].set_title(\"PGD Attack (Ensemble UNet)\")\n",
    "axs[1, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab37104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/486jxbhd03z06w9634qqg4km0000gn/T/ipykernel_19407/2007709468.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  single_unet.load_state_dict(torch.load(\"./weights/cityscapes_unet.pth\"))\n",
      "/var/folders/b6/486jxbhd03z06w9634qqg4km0000gn/T/ipykernel_19407/2007709468.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"./weights/cityscapes_ensemble_unet_{i+1}.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to cityscapes_segmentation_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchattacks\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.Cityscapes.UNetTorchCityscapes import UNet\n",
    "from sklearn.metrics import jaccard_score, f1_score, accuracy_score\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "single_unet = UNet(in_channels=3, out_channels=34)\n",
    "single_unet.load_state_dict(torch.load(\"./weights/cityscapes_unet.pth\"))\n",
    "single_unet.to(device)\n",
    "single_unet.eval()\n",
    "\n",
    "\n",
    "ensemble_models = []\n",
    "for i in range(3):\n",
    "    model = UNet(in_channels=3, out_channels=34)\n",
    "    model.load_state_dict(torch.load(f\"./weights/cityscapes_ensemble_unet_{i+1}.pth\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "\n",
    "fgsm = torchattacks.FGSM(single_unet, eps=0.02)\n",
    "pgd = torchattacks.PGD(single_unet, eps=0.02, alpha=0.01, steps=40)\n",
    "\n",
    "\n",
    "def apply_colormap(mask):\n",
    "    colormap = np.array([\n",
    "        [0, 0, 0], [0, 255, 0], [0, 0, 255],\n",
    "        [255, 0, 0], [255, 255, 0], [255, 0, 255],\n",
    "        [0, 255, 255], [128, 0, 0], [0, 128, 0],\n",
    "        [0, 0, 128], [128, 128, 0], [128, 0, 128],\n",
    "        [0, 128, 128], [64, 0, 0], [0, 64, 0],\n",
    "        [0, 0, 64], [64, 64, 0], [64, 0, 64],\n",
    "        [0, 64, 64], [192, 192, 192], [128, 128, 128],\n",
    "        [64, 128, 128], [128, 64, 128], [128, 128, 64],\n",
    "        [192, 0, 0], [0, 192, 0], [0, 0, 192],\n",
    "        [192, 192, 0], [192, 0, 192], [0, 192, 192],\n",
    "        [64, 64, 64], [32, 32, 32], [160, 160, 160],\n",
    "        [224, 224, 224]\n",
    "    ])\n",
    "    return colormap[mask % len(colormap)].astype(np.uint8)\n",
    "\n",
    "\n",
    "def compute_metrics(pred, gt, num_classes=34):\n",
    "    pred_flat = pred.flatten()\n",
    "    gt_flat = gt.flatten()\n",
    "\n",
    "    iou = jaccard_score(gt_flat, pred_flat, average='macro', labels=range(num_classes), zero_division=0)\n",
    "    dice = f1_score(gt_flat, pred_flat, average='macro', labels=range(num_classes), zero_division=0)\n",
    "    pixel_acc = accuracy_score(gt_flat, pred_flat)\n",
    "    return dice, iou, pixel_acc\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "val_iter = iter(val_loader)\n",
    "for i in range(20):\n",
    "    sample_image, sample_target = next(val_iter)\n",
    "    sample_image = sample_image.to(device)\n",
    "    sample_target = sample_target.to(device).long().squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        single_out = single_unet(sample_image)\n",
    "        single_pred = torch.argmax(single_out, dim=1)\n",
    "\n",
    "        ensemble_probs = [torch.softmax(m(sample_image), dim=1) for m in ensemble_models]\n",
    "        ensemble_avg = torch.stack(ensemble_probs).mean(0)\n",
    "        ensemble_pred = torch.argmax(ensemble_avg, dim=1)\n",
    "\n",
    "    sample_fgsm = fgsm(sample_image, sample_target)\n",
    "    sample_pgd = pgd(sample_image, sample_target)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fgsm_single_pred = torch.argmax(single_unet(sample_fgsm), dim=1)\n",
    "        pgd_single_pred = torch.argmax(single_unet(sample_pgd), dim=1)\n",
    "\n",
    "        fgsm_ensemble_avg = torch.stack([torch.softmax(m(sample_fgsm), dim=1) for m in ensemble_models]).mean(0)\n",
    "        pgd_ensemble_avg = torch.stack([torch.softmax(m(sample_pgd), dim=1) for m in ensemble_models]).mean(0)\n",
    "\n",
    "        fgsm_ensemble_pred = torch.argmax(fgsm_ensemble_avg, dim=1)\n",
    "        pgd_ensemble_pred = torch.argmax(pgd_ensemble_avg, dim=1)\n",
    "\n",
    "    modes = {\n",
    "        \"Single\": single_pred,\n",
    "        \"Ensemble\": ensemble_pred,\n",
    "        \"FGSM_Single\": fgsm_single_pred,\n",
    "        \"PGD_Single\": pgd_single_pred,\n",
    "        \"FGSM_Ensemble\": fgsm_ensemble_pred,\n",
    "        \"PGD_Ensemble\": pgd_ensemble_pred\n",
    "    }\n",
    "\n",
    "    for mode_name, pred_mask in modes.items():\n",
    "        dice, iou, acc = compute_metrics(pred_mask[0].cpu().numpy(), sample_target[0].cpu().numpy())\n",
    "        results.append({\n",
    "            \"Sample\": i,\n",
    "            \"Mode\": mode_name,\n",
    "            \"Dice\": dice,\n",
    "            \"IoU\": iou,\n",
    "            \"PixelAccuracy\": acc\n",
    "        })\n",
    "\n",
    "    if i < 3:\n",
    "        img = sample_image[0].cpu().permute(1, 2, 0).numpy()\n",
    "        gt = sample_target[0].cpu().numpy()\n",
    "\n",
    "        fig, axs = plt.subplots(2, 4, figsize=(18, 9))\n",
    "        axs[0, 0].imshow(img)\n",
    "        axs[0, 0].set_title(\"Input Image\")\n",
    "        axs[0, 0].axis(\"off\")\n",
    "\n",
    "        axs[0, 1].imshow(apply_colormap(gt))\n",
    "        axs[0, 1].set_title(\"GT\")\n",
    "        axs[0, 1].axis(\"off\")\n",
    "\n",
    "        axs[0, 2].imshow(apply_colormap(modes[\"Single\"][0].cpu().numpy()))\n",
    "        axs[0, 2].set_title(\"Single UNet\")\n",
    "        axs[0, 2].axis(\"off\")\n",
    "\n",
    "        axs[0, 3].imshow(apply_colormap(modes[\"Ensemble\"][0].cpu().numpy()))\n",
    "        axs[0, 3].set_title(\"Ensemble UNet\")\n",
    "        axs[0, 3].axis(\"off\")\n",
    "\n",
    "        axs[1, 0].imshow(apply_colormap(modes[\"FGSM_Single\"][0].cpu().numpy()))\n",
    "        axs[1, 0].set_title(\"FGSM + Single\")\n",
    "        axs[1, 0].axis(\"off\")\n",
    "\n",
    "        axs[1, 1].imshow(apply_colormap(modes[\"PGD_Single\"][0].cpu().numpy()))\n",
    "        axs[1, 1].set_title(\"PGD + Single\")\n",
    "        axs[1, 1].axis(\"off\")\n",
    "\n",
    "        axs[1, 2].imshow(apply_colormap(modes[\"FGSM_Ensemble\"][0].cpu().numpy()))\n",
    "        axs[1, 2].set_title(\"FGSM + Ensemble\")\n",
    "        axs[1, 2].axis(\"off\")\n",
    "\n",
    "        axs[1, 3].imshow(apply_colormap(modes[\"PGD_Ensemble\"][0].cpu().numpy()))\n",
    "        axs[1, 3].set_title(\"PGD + Ensemble\")\n",
    "        axs[1, 3].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"sample_{i}_viz.png\")\n",
    "        plt.close()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"cityscapes_segmentation_metrics.csv\", index=False)\n",
    "print(\"Saved results to cityscapes_segmentation_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01195406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved detailed metrics and summary to CSV files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"cityscapes_segmentation_metrics.csv\")\n",
    "\n",
    "summary_df = df.groupby(\"Mode\")[[\"Dice\", \"IoU\", \"PixelAccuracy\"]].mean().reset_index()\n",
    "summary_df.to_csv(\"cityscapes_segmentation_metrics_summary.csv\", index=False)\n",
    "\n",
    "with open(\"cityscapes_segmentation_metrics.csv\", \"a\") as f:\n",
    "    f.write(\"\\n# Summary (Average metrics per mode)\\n\")\n",
    "    summary_df.to_csv(f, index=False)\n",
    "\n",
    "print(\"Saved detailed metrics and summary to CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "526bbaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mode</th>\n",
       "      <th>Dice</th>\n",
       "      <th>IoU</th>\n",
       "      <th>PixelAccuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>0.200513</td>\n",
       "      <td>0.174177</td>\n",
       "      <td>0.872455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FGSM_Ensemble</td>\n",
       "      <td>0.137818</td>\n",
       "      <td>0.108745</td>\n",
       "      <td>0.655298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FGSM_Single</td>\n",
       "      <td>0.098644</td>\n",
       "      <td>0.071146</td>\n",
       "      <td>0.383289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PGD_Ensemble</td>\n",
       "      <td>0.138914</td>\n",
       "      <td>0.106981</td>\n",
       "      <td>0.635577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PGD_Single</td>\n",
       "      <td>0.049675</td>\n",
       "      <td>0.033256</td>\n",
       "      <td>0.177698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Single</td>\n",
       "      <td>0.194715</td>\n",
       "      <td>0.167594</td>\n",
       "      <td>0.852722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mode      Dice       IoU  PixelAccuracy\n",
       "0       Ensemble  0.200513  0.174177       0.872455\n",
       "1  FGSM_Ensemble  0.137818  0.108745       0.655298\n",
       "2    FGSM_Single  0.098644  0.071146       0.383289\n",
       "3   PGD_Ensemble  0.138914  0.106981       0.635577\n",
       "4     PGD_Single  0.049675  0.033256       0.177698\n",
       "5         Single  0.194715  0.167594       0.852722"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
